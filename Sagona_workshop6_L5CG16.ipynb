{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAmuMDz9CUIrO9YIuaV0+7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SagonaAchhami/5CS037/blob/main/Sagona_workshop6_L5CG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g8pvO409lWoi"
      },
      "outputs": [],
      "source": [
        "#1\n",
        "def logistic_function(x):\n",
        "    \"\"\"\n",
        "    Computes the logistic function applied to any value of x.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    y = 1 / (1 + np.exp(-x))\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test logistic function\n",
        "import numpy as np\n",
        "\n",
        "def test_logistic_function():\n",
        "    x_scalar = 0\n",
        "    expected_output_scalar = round(1 / (1 + np.exp(0)), 3)\n",
        "    assert round(logistic_function(x_scalar), 3) == expected_output_scalar\n",
        "\n",
        "    x_pos = 2\n",
        "    expected_output_pos = round(1 / (1 + np.exp(-2)), 3)\n",
        "    assert round(logistic_function(x_pos), 3) == expected_output_pos\n",
        "\n",
        "    x_neg = -3\n",
        "    expected_output_neg = round(1 / (1 + np.exp(3)), 3)\n",
        "    assert round(logistic_function(x_neg), 3) == expected_output_neg\n",
        "\n",
        "    x_array = np.array([0, 2, -3])\n",
        "    expected_output_array = np.array([0.5, 0.881, 0.047])\n",
        "    assert np.all(np.round(logistic_function(x_array), 3) == expected_output_array)\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_logistic_function()\n"
      ],
      "metadata": {
        "id": "WSSCA3LimiVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a47d1ae2-b079-4c31-f303-b0688545ffdb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "def log_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes log loss.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "    loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "XO4Cg2g4miaL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function:\n",
        "y_true, y_pred = 0, 0.1\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')\n",
        "print(\"+++++++++++++--------------------------++++++++++++++++++++++++\")\n",
        "y_true, y_pred = 1, 0.9\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')"
      ],
      "metadata": {
        "id": "4bX0kCQbs2g_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb793850-1bc5-47f2-9a3f-e1bfd2c2a21c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log loss(0, 0.1) ==> 0.10536051565782628\n",
            "+++++++++++++--------------------------++++++++++++++++++++++++\n",
            "log loss(1, 0.9) ==> 0.10536051565782628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_log_loss():\n",
        "    import numpy as np\n",
        "\n",
        " # Test case 1: Perfect prediction (y_true = 1, y_pred = 1)\n",
        "y_true = 1\n",
        "y_pred = 1\n",
        "expected_loss = 0.0 # Log loss is 0 for perfect prediction\n",
        "assert np.isclose(log_loss(y_true, y_pred), expected_loss), \"Test failed for perfect prediction (y_true=1, y_pred=1)\"\n",
        "# Test case 2: Perfect prediction (y_true = 0, y_pred = 0)\n",
        "y_true = 0\n",
        "y_pred = 0\n",
        "expected_loss = 0.0 # Log loss is 0 for perfect prediction\n",
        "assert np.isclose(log_loss(y_true, y_pred), expected_loss), \"Test failed for perfect prediction (y_true=0, y_pred=0)\"\n",
        "# Test case 3: Incorrect prediction (y_true = 1, y_pred = 0)\n",
        "y_true = 1\n",
        "y_pred = 0\n",
        "try:\n",
        "  log_loss(y_true, y_pred) # This should raise an error due to log(0)\n",
        "except ValueError:\n",
        "  pass # Test passed if ValueError is raised for log(0)\n",
        "# Test case 4: Incorrect prediction (y_true = 0, y_pred = 1)\n",
        "y_true = 0\n",
        "y_pred = 1\n",
        "try:\n",
        "  log_loss(y_true, y_pred) # This should raise an error due to log(0)\n",
        "except ValueError:\n",
        "  pass # Test passed if ValueError is raised for log(0)\n",
        "# Test case 5: Partially correct prediction\n",
        "y_true = 1\n",
        "y_pred = 0.8\n",
        "expected_loss = -(1 * np.log(0.8)) - (0 * np.log(0.2)) # ~0.2231\n",
        "assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), \"Test failed for partiallycorrect prediction (y_true=1, y_pred=0.8)\"\n",
        "y_true = 0\n",
        "y_pred = 0.2\n",
        "expected_loss = -(0 * np.log(0.2)) - (1 * np.log(0.8)) # ~0.2231\n",
        "assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), \"Test failed for partially correct prediction (y_true=0, y_pred=0.2)\"\n",
        "print(\"All tests passed!\")\n",
        "# Run the test case\n",
        "test_log_loss()"
      ],
      "metadata": {
        "id": "g7k7_XbPmidi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ae5123-93d0-4fc9-e326-51517cda440e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "def cost_function(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes average log loss.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    y_true = np.asarray(y_true).reshape(-1)\n",
        "    y_pred = np.asarray(y_pred).reshape(-1)\n",
        "\n",
        "    assert len(y_true) == len(y_pred), \"y_true and y_pred must have the same length\"\n",
        "\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "\n",
        "    loss = -(y_true * np.log(y_pred)) - ((1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "    # Average the loss\n",
        "    cost = np.sum(loss) / len(y_true)\n",
        "\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "qLIcItcZmiiH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_cost_function():\n",
        "    import numpy as np\n",
        "\n",
        "    y_true = np.array([1, 0, 1])\n",
        "    y_pred = np.array([0.9, 0.1, 0.8])\n",
        "\n",
        "    expected_cost = (-(1 * np.log(0.9)) - (1 - 1) * np.log(1 - 0.9) +\n",
        "                     -(0 * np.log(0.1)) - (1 - 0) * np.log(1 - 0.1) +\n",
        "                     -(1 * np.log(0.8)) - (1 - 1) * np.log(1 - 0.8)) / 3\n",
        "\n",
        "    result = cost_function(y_true, y_pred)\n",
        "    assert np.isclose(result, expected_cost, atol=1e-6), f\"Test failed: {result} != {expected_cost}\"\n",
        "    print(\"Test passed!\")\n",
        "\n",
        "test_cost_function()\n"
      ],
      "metadata": {
        "id": "aiEvEtTSmimD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b68c059-7625-4ff8-fbfe-b6d098f9cca3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "def costfunction_logreg(X, y, w, b):\n",
        "    n, d = X.shape\n",
        "    assert len(y) == n\n",
        "    assert len(w) == d\n",
        "\n",
        "    z = np.dot(X, w) + b\n",
        "    y_pred = logistic_function(z)\n",
        "    cost = cost_function(y, y_pred)\n",
        "    return cost\n",
        "    # Testing the Function:\n",
        "X, y, w, b = np.array([[10, 20], [-10, 10]]), np.array([1, 0]), np.array([0.5, 1.5]), 1\n",
        "print(f\"cost for logistic regression(X = {X}, y = {y}, w = {w}, b = {b}) = {costfunction_logreg(X, y, w, b)}\")\n"
      ],
      "metadata": {
        "id": "SEDEZrBlmipT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52cc247-b558-4dab-d49c-039b4d4787d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost for logistic regression(X = [[ 10  20]\n",
            " [-10  10]], y = [1 0], w = [0.5 1.5], b = 1) = 5.500008350834906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "def compute_gradient(X, y, w, b):\n",
        "    n, d = X.shape\n",
        "    assert len(y) == n\n",
        "    assert len(w) == d\n",
        "\n",
        "    y_pred = logistic_function(np.dot(X, w) + b)\n",
        "\n",
        "    grad_w = np.dot(X.T, (y_pred - y)) / n\n",
        "    grad_b = np.sum(y_pred - y) / n\n",
        "\n",
        "    return grad_w, grad_b\n"
      ],
      "metadata": {
        "id": "7eE1OObJmisB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple test case\n",
        "X = np.array([[10, 20], [-10, 10]]) # shape (2, 2)\n",
        "y = np.array([1, 0]) # shape (2,)\n",
        "w = np.array([0.5, 1.5]) # shape (2,)\n",
        "b = 1 # scalar\n",
        "# Assertion tests\n",
        "try:\n",
        "  grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "  print(\"Gradients computed successfully.\")\n",
        "  print(f\"grad_w: {grad_w}\")\n",
        "  print(f\"grad_b: {grad_b}\")\n",
        "except AssertionError as e:\n",
        "  print(f\"Assertion error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSu7FqjS7IIU",
        "outputId": "1bbffda6-f7c1-4f90-dd49-e182ae7f9536"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients computed successfully.\n",
            "grad_w: [-4.99991649  4.99991649]\n",
            "grad_b: 0.4999916492890759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "def gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=True):\n",
        "\n",
        "    cost_history = []\n",
        "    params_history = []\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "\n",
        "        w -= alpha * grad_w\n",
        "        b -= alpha * grad_b\n",
        "\n",
        "        cost = costfunction_logreg(X, y, w, b)\n",
        "\n",
        "        cost_history.append(cost)\n",
        "        params_history.append((w.copy(), b))\n",
        "\n",
        "\n",
        "    return w, b, cost_history, params_history\n"
      ],
      "metadata": {
        "id": "tPwwICtHmiwK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the gradient_descent function with sample data\n",
        "X = np.array([[0.1, 0.2], [-0.1, 0.1]]) # Shape (2, 2)\n",
        "y = np.array([1, 0]) # Shape (2,)\n",
        "w = np.zeros(X.shape[1]) # Shape (2,) - same as number of features\n",
        "b = 0.0 # Scalar\n",
        "alpha = 0.1 # Learning rate\n",
        "n_iter = 100000 # Number of iterations\n",
        "# Perform gradient descent\n",
        "w_out, b_out, cost_history, params_history = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=True,show_params=False)\n",
        "# Print final parameters and cost\n",
        "print(\"\\nFinal parameters:\")\n",
        "print(f\"w: {w_out}, b: {b_out}\")\n",
        "print(f\"Final cost: {cost_history[-1]:.6f}\")\n",
        "\n",
        "\"\"\"A simple assertion test for gradient descent Function:\"\"\"\n",
        "\n",
        "# Simple assertion test for gradient_descent\n",
        "def test_gradient_descent():\n",
        "  X = np.array([[0.1, 0.2], [-0.1, 0.1]]) # Shape (2, 2)\n",
        "  y = np.array([1, 0]) # Shape (2,)\n",
        "  w = np.zeros(X.shape[1]) # Shape (2,)\n",
        "  b = 0.0 # Scalar\n",
        "  alpha = 0.1 # Learning rate\n",
        "  n_iter = 100 # Number of iterations\n",
        "# Run gradient descent\n",
        "  w_out, b_out, cost_history, _ = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False,show_params=False)\n",
        "# Assertions\n",
        "  assert len(cost_history) == n_iter, \"Cost history length does not match the number of iterations\"\n",
        "  assert w_out.shape == w.shape, \"Shape of output weights does not match the initial weights\"\n",
        "  assert isinstance(b_out, float), \"Bias output is not a float\"\n",
        "  assert cost_history[-1] < cost_history[0], \"Cost did not decrease over iterations\"\n",
        "  print(\"All tests passed!\")\n",
        "# Run the test\n",
        "test_gradient_descent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFZykb_i6aJK",
        "outputId": "b5a26e36-e9f6-42d3-f4f0-7cdfcabe3c0d",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final parameters:\n",
            "w: [38.51304248 18.83386869], b: -2.8176836626325836\n",
            "Final cost: 0.008254\n",
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "def prediction(X, w, b, threshold=0.5):\n",
        "  y_test_prob = logistic_function(np.dot(X, w) + b)\n",
        "  y_pred = (y_test_prob >= threshold).astype(int)\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "bG7WshKtmizw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prediction():\n",
        "  X_test = np.array([[0.5, 1.0], [1.5, -0.5], [-0.5, -1.0]]) # Shape (3, 2)\n",
        "  w_test = np.array([1.0, -1.0]) # Shape (2,)\n",
        "  b_test = 0.0 # Scalar bias\n",
        "  threshold = 0.5 # Default threshold\n",
        "# Updated expected output\n",
        "  expected_output = np.array([0, 1, 1])\n",
        "# Call the prediction function\n",
        "  y_pred = prediction(X_test, w_test, b_test, threshold)\n",
        "# Assert that the output matches the expected output\n",
        "  assert np.array_equal(y_pred, expected_output), f\"Expected {expected_output}, but got {y_pred}\"\n",
        "  print(\"Test passed!\")\n",
        "test_prediction()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYGR4LNY5X2w",
        "outputId": "2b9141aa-9cd9-47d5-f9c2-d2b8da51aa24"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8\n",
        "def evaluate_classification(y_true, y_pred):\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    confusion_matrix = np.array([[TN, FP],\n",
        "                                  [FN, TP]])\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    return confusion_matrix, precision, recall, f1_score\n"
      ],
      "metadata": {
        "id": "UI9at-tCmi3l"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}